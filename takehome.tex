\documentclass{article}
\usepackage{lmodern}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}
\usepackage{enumitem}
\usepackage{amsmath}

\title{Machine Learning End Term Exam}
\author{Ahmad Shayaan IMT2014004 \\ H Vijaya Sharvani IMT2014022 \\ Indu Ilanchezian IMT2014024}


\begin{document}
\maketitle
\pagenumbering{gobble}
\pagenumbering{arabic}

\section*{Question 2}

To derive the solution to the modified linear regression leads to the generalized form of ridge regression.
\\
\\
Solution:-
\\
\\
Given the attribute $x_i = \hat{x_i} + \epsilon_i $, where the $\hat{x_i}$ are the true measurements and $\epsilon_i$ is the zero mean vector with covariance matrix $\sigma^2 I$
\\
Modified loss function
\begin{equation*}
W^* = argmin_w E_\epsilon\sum_{i=1}^{n}(y_i - W^T(\hat{x_i} + \epsilon_i))^2
\end{equation*}
Where W is the transformation vector.
\begin{equation}
W^* = argmin_W E_\epsilon || Y - (X+\epsilon)W||_2^2 \tag{1}
\end{equation}
\\
Where
\begin{equation*}
	Y = \begin{bmatrix}
	y_1\\y_2\\\vdots\\y_n 
	\end{bmatrix}
\end{equation*}

\begin{equation*}
	X = \begin{bmatrix}
	\hat{x}_1^T\\\hat{x}_2^T\\\vdots\\\hat{x}_n^T
	\end{bmatrix}
\end{equation*}

\begin{equation*}
	\epsilon = \begin{bmatrix}
		\epsilon_1^T\\\epsilon_2^T\\\vdots\\\epsilon_n^T
	\end{bmatrix}
\end{equation*}

Expanding right hand side of equation 1.
\begin{equation*}
 E_\epsilon || Y - (X+\epsilon)W||_2^2  = E_\epsilon \bigg[(Y - (X+\epsilon)W)^T(Y - (X+\epsilon)W)\bigg]
\end{equation*}
\begin{equation*}
= E_\epsilon \bigg[ Y^TY + W^T(X+\epsilon)^T(X+E) - 2W^T(X+E)^TY \bigg] \tag{2}
\end{equation*}
To minimize the equation we will differentiate eq 2 wrt W.
\begin{equation*}
	\frac{\partial E_\epsilon \bigg[ Y^TY + W^T(X+\epsilon)^T(X+\epsilon)W - 2W^T(X+E)^TY \bigg]}{\partial W} = 0
\end{equation*}
We know that $\frac{\partial E(f(x))}{\partial}$  = $E\frac{\partial f(x)}{\partial x}$.

\begin{equation*}
	E_\epsilon \bigg[ \frac{\partial Y^TY}{\partial W } + \frac{\partial W^T(X+\epsilon)^T(X+\epsilon)W}{\partial W} -  2\frac{\partial W^T(X+E)^TY}{\partial W}\bigg] = 0
\end{equation*}

\begin{equation*}
	E\epsilon\big[ 2(X+\epsilon)^T(X+\epsilon)W -2(X+\epsilon)^TY\big] = 0
\end{equation*}

\begin{equation*}
	2E_\epsilon \big[(X+\epsilon)^T(X+\epsilon)W \big] -2 E_\epsilon \big[(X+\epsilon)^TY\big] = 0
\end{equation*}

\begin{equation*}
	E_\epsilon\big[(X^TX+\epsilon^T\epsilon +2\epsilon^TX\big)W] = E_\epsilon\big[(X+\epsilon)^TY\big])
\end{equation*}
\begin{equation*}
	E_\epsilon(X^TXW)+ E_\epsilon(\epsilon^T\epsilon W) + 2E_\epsilon(\epsilon^TX W) = E_\epsilon(X^TY) + E_\epsilon(\epsilon^TY)
\end{equation*}
We know that E(AB) = E(A)E(B) if A and B are independent variables and $E_f(h(x)) = \int_{-\infty}^{\infty}h(x)f(x)dx$.

\begin{equation*}
	\sum_{i=1}^{n}X^TXWP(\epsilon_i) + E_\epsilon(\epsilon\epsilon^T)E_\epsilon(W) +2E_\epsilon(X)E_\epsilon(\epsilon)  = \sum_{i=1}^{n} X^TYP(\epsilon_i) + E_\epsilon(Y)E_\epsilon(\epsilon)
\end{equation*}
We know that the noise is a zero mean Gaussian noise therefore E($\epsilon$) = 0
\begin{equation*}
	(X^TX + \sigma^2I)W = X^TY
\end{equation*}
\begin{equation*}
	W = (X^TX + \sigma^2I)^{-1}X^TY
\end{equation*}

therefore the solution of the minimization is 
\begin{equation*}
		W^* = (X^TX + \sigma^2I)^{-1}X^TY
\end{equation*}
This solution is same as the solution for Ridge regression
\begin{equation*}
	W^* = (X^TX  + \lambda I)^{-1}X^TY
\end{equation*}

\section*{Question 5}
 
Let $\vec{x_{1}}, \vec{x_{2}} ... \vec{x_{n}}$ be the feature vectors of $n$ data points in the original feature space. Let $\phi$ be the feature tranformation function. Then, $\phi(\vec{x_{1}}), \phi(\vec{x_{2}}) ..., \phi(\vec{x_{n}})$ are the feature vectors in the transformed feature space.  
\\
Let K be the kernel function such that:
\begin{equation*}
K(i,j) = \phi(x_{i})^T \phi(x_{j})
\end{equation*} 
\\
The center of mass, $\vec{\mu}$, in the feature space can be defined as the average of the vectors in the transformed feature space. 
\begin{equation*}
\vec{\mu} = \frac{1}{n}\sum_{i=1}^{n} \phi(\vec{x_{i}})
\end{equation*}
\\
Consider:
\begin{equation*}
\begin{split}
||{\mu}||^{2} &= \mu^T\mu \\ 
              &= \mu^T \frac{1}{n}\sum_{i=1}^{n} \phi(\vec{x_{i}}) \\
              &= \frac{1}{n}\sum_{j=1}^{n} \phi(\vec{x_{j}})^T \frac{1}{n}\sum_{i=1}^{n} \phi(\vec{x_{i}}) \\
              &= \frac{1}{n^2} \sum_{i,j} \phi(\vec{x_{j}})^{T}) \phi(\vec{x_{i}}) \\
              &= \frac{1}{n^2} \sum_{i,j} K(i,j)
\end{split}
\end{equation*}

\subsection*{Average of the squared Euclidean distances from $\mu$ to each $\phi(x)$} 

The squared euclidean distance of a single feature vector in the transformed space from the center of mass $\vec{\mu}$ can be expressed as follows:
\\
\begin{equation*}
\begin{split}
||\phi\vec{x_{i}}) - \vec{\mu}||^2 &= (\phi(\vec{x_{i}}) - \vec{\mu})^T(\phi(\vec{x_{i}}) - \vec{\mu}) \\
&= \phi(\vec{x_{i}})^T\phi(\vec{x_{i}}) - 2\phi(\vec{x_{i}})^T\vec{\mu} + ||\vec{\mu}||^2 \\
&= K(i,i) - \frac{2}{n}\phi(\vec{x_{i}})^T \sum_{j=1}^{n} \phi(\vec{x_{j}}) + || \vec{\mu}||^2 \\
&= K(i,i) - \frac{2}{n} \sum_{j=1}^{n} \phi(\vec{x_{i}})^T \phi(\vec{x_{j}}) + || \vec{\mu}||^2 \\
&= K(i,i) - \frac{2}{n} \sum_{j=1}^{n} K(i,j) + \frac{1}{n^2}\sum_{r,s} K(r,s) \\
\end{split}
\end{equation*}
\\
The average of the euclidean distances of all the points from the center of mass can be written as:
\\
\begin{equation*}
\begin{split}
\frac{1}{n} \sum_{i=1}^{n} ||\phi(\vec{x_{i}})-\vec{\mu}||^2 &= \frac{1}{n} \Bigg(\sum_{i=1}^{n} \bigg( K(i,i) - \frac{2}{n} \sum_{j=1}^{n} K(i,j) + \frac{1}{n^2} \sum_{r,s} K(r,s) \bigg) \Bigg)  \\
&= \frac{1}{n}\bigg( \sum_{i=1}^{n} K(i,i) - \frac{2}{n} \sum_{i,j} K(i,j) +\frac{n}{n^2} \sum_{r,s} K(r,s) \bigg) \\
&= \frac{1}{n} \bigg( \sum_{i=1}^{n} K(i,i) - \frac{1}{n} \sum_{i,j} K(i,j) \bigg) 
\end{split}
\end{equation*}
\\
Thus, the average of euclidean distances from the center of mass $\vec{\mu}$ to each $\phi(x)$ can be expressed in terms of the kernel function $K$.

\end{document}